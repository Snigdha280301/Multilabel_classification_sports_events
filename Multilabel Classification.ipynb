{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc49e6b0-cb01-4fba-82e7-11e11707099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\snigd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\snigd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Processing files: 100%|██████████| 3840/3840 [00:06<00:00, 620.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Commentary\n",
      "0  man claiming associate Chinese tennis star Pen...\n",
      "1  Lennon brands Rangers favourites Celtics Neil ...\n",
      "2  Nigerias Blessing Oborududu advanced Final wom...\n",
      "3  Gerrard happy Anfield Liverpool captain Steven...\n",
      "4  Williams battles Aussie title Serena Williams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments, pipeline, BertConfig\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import spacy\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Step 1: Load and Preprocess the Data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Directory containing the .txt files\n",
    "DATA_DIR = \"C:/Users/snigd/Downloads/train_data\"\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove non-word characters and extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenize words and remove stopwords\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    preprocessed_text = ' '.join(words)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Function to load and preprocess the dataset file-by-file\n",
    "def load_dataset(data_dir):\n",
    "    all_texts = []\n",
    "    # Use tqdm to add a progress bar for file processing\n",
    "    for filename in tqdm(os.listdir(data_dir), desc=\"Processing files\"):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    preprocessed_text = preprocess_text(text)\n",
    "                    all_texts.append(preprocessed_text)  # Append the entire preprocessed text\n",
    "            except UnicodeDecodeError:\n",
    "                with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                    text = file.read()\n",
    "                    preprocessed_text = preprocess_text(text)\n",
    "                    all_texts.append(preprocessed_text)  # Append the entire preprocessed text\n",
    "    return all_texts\n",
    "\n",
    "# Load and preprocess the dataset file-by-file with progress tracking\n",
    "texts = load_dataset(DATA_DIR)\n",
    "\n",
    "# Assuming all preprocessed texts are loaded into a DataFrame\n",
    "data = pd.DataFrame({'Commentary': texts})\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7defd1-6e81-4ec8-a54c-25cda65f4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  get BERT embeddings for a given text\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embedding.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce00ec1c-151c-482a-ac37-3603e75a18b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering: 3840\n"
     ]
    }
   ],
   "source": [
    "# Define a list of sport names for extraction\n",
    "sport_names = [\"soccer/football\", \"american football\", \"basketball\", \"cricket\", \"tennis\", \"baseball\", \"athletics\"]\n",
    "\n",
    "# Labels for multi-label encoding\n",
    "label_list = [\n",
    "    \"score-related\", \n",
    "    \"assist/playmaking\", \n",
    "    \"foul/penalty\",  \n",
    "    \"substitution/injury\", \n",
    "    \"defense actions\",  \n",
    "    \"game outcome\" \n",
    "]\n",
    "\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "contextual_words = {\n",
    "    \"soccer/football\": [\n",
    "        \"goal post\", \"corner\", \"free kick\", \"penalty\", \"offside\", \n",
    "        \"crossbar\", \"goal\", \"netted\", \"assist\", \"yellow card\", \n",
    "        \"red card\", \"goalkeeper\", \"defender\", \"midfielder\", \"striker\", \"winger\",\n",
    "        \"goal\", \"header\", \"equalizer\", \"stoppage time\", \"penalty shootout\",\n",
    "        \"counter-attack\", \"corner kick\", \"soccer\", \"FIFA\", \"UEFA Champions League\"\n",
    "    ],\n",
    "    \"basketball\": [\n",
    "        \"hoop\", \"three-pointer\", \"dribble\", \"slam dunk\", \"free throw\", \"dribble\", \"guard\",\n",
    "        \"alley-oop\", \"backboard\", \"dunk\", \"rebound\", \"assist\", \"pass\", \"block\",\n",
    "        \"steal\", \"turnover\", \"fast break\", \"point guard\", \"shooting guard\", \n",
    "        \"small forward\", \"power forward\", \"center\", \"layup\", \"buzzer-beater\",\n",
    "        \"man-to-man defense\", \"zone defense\", \"basketball\", \"NBA\", \"NCAA\"\n",
    "    ],\n",
    "    \"american football\": [\n",
    "        \"end zone\", \"field goal\", \"touchdown\", \"huddle\", \"scrimmage\", \n",
    "        \"quarterback sneak\", \"line of scrimmage\", \"sack\", \"fumble\", \"punt\", \n",
    "        \"interception\", \"pass interference\", \"tackle\", \"field goal\", \"tackle\",\n",
    "        \"quarterback\", \"running back\", \"wide receiver\", \"linebacker\", \"kickoff\",\n",
    "        \"american football\", \"NFL\", \"football\", \"american football\", \"Super Bowl\"\n",
    "    ],\n",
    "    \"cricket\": [\n",
    "        \"pitch\", \"crease\", \"boundary\", \"overs\", \"wicket\", \"innings\", \"bowling\", \"batting\", \n",
    "        \"stump\", \"run rate\", \"batsman\", \"bowler\", \"fielder\", \"wicketkeeper\",\n",
    "        \"run out\", \"century\", \"no ball\", \"wide ball\", \"catch out\", \"duck\",\n",
    "        \"maiden over\", \"leg spin\", \"off spin\", \"googly\", \"reverse swing\", \"cricket\",\n",
    "        \"ICC Cricket World Cup\", \"Indian Premier League (IPL)\", \"ICC T20 World Cup\"\n",
    "    ],\n",
    "    \"tennis\": [\n",
    "        \"court\", \"serve\", \"racket\", \"volley\", \"backhand\", \"forehand\", \"deuce\", \"set\", \"singles\", \"doubles\",\n",
    "        \"advantage\", \"tiebreak\", \"ace\", \"match point\", \"double fault\", \"break point\",\n",
    "        \"server\", \"receiver\", \"baseline\", \"drop shot\", \"lob\", \"grand slam\", \"grass court\", \n",
    "        \"tennis\", \"Wimbledon\", \"US Open\", \"French Open\", \"Australian Open\", \"rally\"\n",
    "   ],\n",
    "    \"baseball\": [\n",
    "        \"pitcher's mound\", \"home plate\", \"strike zone\", \"innings\", \n",
    "        \"dugout\", \"double play\", \"home run\", \"pitcher\", \"catcher\", \"baseman\",\n",
    "        \"outfielder\", \"strikeout\", \"hit\", \"bunt\", \"curveball\", \"fastball\",\n",
    "        \"walk-off\", \"line drive\", \"baseball\", \"World Series\", \"Nippon Series\", \n",
    "         \"College World Series\", \"World Baseball Classic (WBC)\"\n",
    "    ],\n",
    "    \"athletics\": [\n",
    "        \"track\", \"lane\", \"field\", \"relay\", \"javelin\", \"high jump\", \"long jump\", \n",
    "        \"marathon\", \"hurdle\", \"race\", \"sprint\", \"throw\", \"jump\", \"swim\", \"dive\", \"gymnastics\",\n",
    "        \"sprinter\", \"marathoner\", \"runner\", \"jumper\", \"thrower\", \"hurdler\", \"wrestling\", \n",
    "        \"pole vaulter\", \"shot putter\", \"decathlete\", \"heptathlete\", \"starting block\",\n",
    "        \"photo finish\", \"personal best\", \"athletics\", \"olympics\", \"athlete\", \"Olympics\", \"Commonwealth Games\"\n",
    "    ],\n",
    "   \n",
    "}\n",
    "\n",
    "# Helper function to count and weigh matches\n",
    "def weighted_count_matches(words, text, boost_keywords=None, boost_factor=1):\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word in text:\n",
    "            count += 1\n",
    "            # Apply a higher boost if the word is in boost_keywords\n",
    "            if boost_keywords and word in boost_keywords:\n",
    "                count += boost_factor  # Apply the boost factor more strongly\n",
    "    return count\n",
    "\n",
    "def generate_multi_label(text):\n",
    "    labels = [0] * len(label_list)\n",
    "    text = text.lower()\n",
    "    sports = {}\n",
    "\n",
    "    # Score-Related Events\n",
    "    if weighted_count_matches([\n",
    "        \"goal\", \"netted\", \"touchdown\", \"td\", \"scored\", \"wicket\", \"lbw\", \n",
    "        \"six\", \"boundary\", \"maximum\", \"four\", \"goal line\", \"conversion\", \n",
    "        \"penalty kick\", \"field goal\", \"extra point\", \"try\", \"strike\", \"hit\",\n",
    "        \"home run\", \"grand slam\", \"run\", \"free throw\", \"service game\", \n",
    "        \"inning\", \"shot\", \"swing\", \"drive\", \"smash\", \"overhead\", \"homer\",\n",
    "        \"inning\", \"power play\", \"hat-trick\", \"equalizer\"\n",
    "    ], text) > 1:\n",
    "        labels[label_to_id[\"score-related\"]] = 1\n",
    "\n",
    "    # Assist/Playmaking\n",
    "    if weighted_count_matches([\n",
    "        \"assist\", \"played in\", \"pass\", \"setup\", \"set up\", \"through ball\", \n",
    "        \"cross\", \"assist\", \"feed\", \"layup\", \"pass completion\", \"dummy run\",\n",
    "        \"lob\", \"kick out\", \"drive\", \"fast break\", \"pick and roll\", \"volley\",\n",
    "        \"build-up play\", \"one-two\", \"give-and-go\", \"support\"\n",
    "    ], text, boost_keywords=[\"assist\", \"pass\", \"layup\"], boost_factor=3) > 0:\n",
    "        labels[label_to_id[\"assist/playmaking\"]] = 1\n",
    "\n",
    "    # Foul/Penalty\n",
    "    if weighted_count_matches([\n",
    "        \"foul\", \"penalty\", \"yellow card\", \"booked\", \"red card\", \"sent off\", \n",
    "        \"dismissed\", \"ejection\", \"error\", \"violation\", \"handball\", \"offside\", \n",
    "        \"technical foul\", \"flag\", \"encroachment\", \"false start\", \"holding\",\n",
    "        \"unsportsmanlike conduct\", \"pass interference\", \"illegal contact\",\n",
    "        \"foul trouble\", \"flagrant foul\", \"charging\", \"blocking foul\"\n",
    "    ], text, boost_keywords=[\"foul\", \"penalty\", \"red card\"], boost_factor=3) > 0:\n",
    "        labels[label_to_id[\"foul/penalty\"]] = 1\n",
    "\n",
    "    # Substitution/Injury\n",
    "    if weighted_count_matches([\n",
    "        \"substitution\", \"replaced\", \"injury\", \"hurt\", \"wounded\", \"injuries\", \n",
    "        \"substitute\", \"replacement\", \"cramp\", \"knock\", \"concussion\", \n",
    "        \"withdrawn\", \"hamstring\", \"broken\", \"ligament\", \"pain\", \"horrific\",\n",
    "        \"limping\", \"injury time out\", \"stretchered off\", \"medical\"\n",
    "    ], text) > 0:\n",
    "        labels[label_to_id[\"substitution/injury\"]] = 1\n",
    "\n",
    "    # Defense Actions\n",
    "    if weighted_count_matches([\n",
    "        \"tackle\", \"rebound\", \"save\", \"block\", \"defense\", \"defensive play\", \n",
    "        \"sack\", \"fumble\", \"punt\", \"interception\", \"clearance\", \"block shot\", \n",
    "        \"defensive stop\", \"steal\", \"turnover\", \"forced fumble\", \"goal-line stand\",\n",
    "        \"double play\", \"steal\", \"throw-in\", \"corner\", \"restart\", \"fast break\",\n",
    "        \"pressure\", \"slide tackle\", \"marking\"\n",
    "    ], text, boost_keywords=[\"block\", \"steal\", \"interception\", \"tackle\"], boost_factor=3) > 0:\n",
    "        labels[label_to_id[\"defense actions\"]] = 1\n",
    "\n",
    "    # Game Outcome\n",
    "    if weighted_count_matches([\n",
    "        \"victory\", \"win\", \"secured\", \"dominated\", \"triumph\", \"conquered\", \n",
    "        \"prevailed\", \"loss\", \"defeat\", \"lost\", \"outplayed\", \n",
    "        \"clinched\", \"beat\",  \"narrow win\", \"world record\",\n",
    "        \"draw\", \"stalemate\", \"tie\"\n",
    "    ], text) > 1:\n",
    "        labels[label_to_id[\"game outcome\"]] = 1\n",
    "\n",
    "    # Determine the sport based on contextual words\n",
    "    for sport, words in contextual_words.items():\n",
    "        match_count = weighted_count_matches(words, text)\n",
    "        if match_count > 1: \n",
    "            sports[sport] = match_count\n",
    "\n",
    "    # Sort sports based on the count of matching contextual words (desc)\n",
    "    sorted_sports = sorted(sports.items(), key=lambda x: x[1], reverse=True)\n",
    "    final_sport = sorted_sports[0][0] if sorted_sports else None\n",
    "\n",
    "    return labels, final_sport\n",
    "\n",
    "# Apply the multi-label encoding to the dataset\n",
    "data['labels'], data['sports'] = zip(*data['Commentary'].apply(generate_multi_label))\n",
    "print(f\"Number of rows after filtering: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33cb633b-f395-4d67-870d-7cddf4d007bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering: 2184\n",
      "                                          Commentary              labels  \\\n",
      "1  Lennon brands Rangers favourites Celtics Neil ...  [1, 0, 0, 0, 0, 1]   \n",
      "2  Nigerias Blessing Oborududu advanced Final wom...  [0, 0, 0, 0, 0, 0]   \n",
      "3  Gerrard happy Anfield Liverpool captain Steven...  [1, 0, 0, 0, 0, 0]   \n",
      "4  Williams battles Aussie title Serena Williams ...  [1, 0, 1, 1, 0, 1]   \n",
      "5  Redknapps Saints face Pompey tie New Southampt...  [1, 0, 0, 0, 0, 1]   \n",
      "\n",
      "            sports  \n",
      "1  soccer/football  \n",
      "2        athletics  \n",
      "3  soccer/football  \n",
      "4           tennis  \n",
      "5             None  \n"
     ]
    }
   ],
   "source": [
    "# Convert 'labels' column to string to easily compare with a string representation of [0,0,0,0,0,0]\n",
    "data['labels_str'] = data['labels'].apply(lambda x: str(x))\n",
    "\n",
    "# Filter out rows where 'labels' is [0,0,0,0,0,0] and 'sports' is None or empty\n",
    "data = data[(data['labels_str'] != str([0, 0, 0, 0, 0, 0])) | (data['sports'].notna() & data['sports'].astype(bool))]\n",
    "\n",
    "# Drop the helper column 'labels_str'\n",
    "data = data.drop(columns=['labels_str'])\n",
    "\n",
    "# Checking the resulting DataFrame\n",
    "print(f\"Number of rows after filtering: {len(data)}\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f517ad9-c773-4642-9413-004d804dbd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows labeled 'score-related': 1251\n",
      "Number of rows labeled 'assist/playmaking': 629\n",
      "Number of rows labeled 'foul/penalty': 346\n",
      "Number of rows labeled 'substitution/injury': 842\n",
      "Number of rows labeled 'defense actions': 437\n",
      "Number of rows labeled 'game outcome': 1082\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of rows labeled for each label\n",
    "label_counts = {label: 0 for label in label_list}\n",
    "\n",
    "for labels in data['labels']:\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            label_counts[label_list[i]] += 1\n",
    "\n",
    "# Print the number of rows labeled for each label\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Number of rows labeled '{label}': {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca3929df-dc1e-4cb3-bcea-227e999327d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Using Class Weights\n",
    "labels_df = pd.DataFrame(data['labels'].tolist(), columns=label_list)\n",
    "\n",
    "# Flatten the labels to compute class weights\n",
    "flat_labels = np.array(labels_df.values.flatten())\n",
    "\n",
    "# Step 1: Calculate Initial Class Weights\n",
    "initial_class_weights = compute_class_weight('balanced', classes=np.unique(flat_labels), y=flat_labels)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(initial_class_weights)}\n",
    "# Calculate class weights\n",
    "# Define a threshold below which classes are considered as minority\n",
    "minority_threshold = 500  \n",
    "\n",
    "adjusted_class_weights_dict = class_weights_dict.copy()\n",
    "for label, weight in class_weights_dict.items():\n",
    "    if labels_df[label_list[label]].sum() < minority_threshold:\n",
    "        adjusted_class_weights_dict[label] = weight * 2.5  # Multiply by 2 to make weights more aggressive\n",
    "\n",
    "adjusted_class_weights_tensor = torch.tensor(list(adjusted_class_weights_dict.values())).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a3072b2-e784-400b-9ebb-3f2d49a2b0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30dd673985914536a1c3b2715d2fbacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1638 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879734a0784b484ead4d967c48ad432a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/546 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Step 4: Convert the dataset to Dataset objects\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Commentary'], labels_df, test_size=0.25, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_dict({'text': X_train.tolist(), 'label': y_train.values.tolist()})\n",
    "test_dataset = Dataset.from_dict({'text': X_test.tolist(), 'label': y_test.values.tolist()})\n",
    "\n",
    "# Tokenization function\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "def tokenize_and_format(batch):\n",
    "    tokenized_input = tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n",
    "    labels = torch.tensor(batch['label'], dtype=torch.float32)  # Ensure labels are float tensors for BCEWithLogitsLoss\n",
    " # Long tensor for multi-class labels\n",
    "    return {**tokenized_input, 'labels': labels}\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_format, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_format, batched=True)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0598e5cf-d1bb-4115-8788-692d95725d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snigd\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load pre-trained model configuration and modify it\n",
    "config = BertConfig.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    num_labels=len(label_list),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    hidden_dropout_prob=0.3,  \n",
    "    attention_probs_dropout_prob=0.3,  \n",
    "    num_hidden_layers=12  \n",
    ")\n",
    "\n",
    "# Load the modified BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased', config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e80f4bca-f9bc-4c7e-baa2-283c60ac1a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snigd\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\snigd\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "C:\\Users\\snigd\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='765' max='765' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [765/765 07:44, Epoch 14/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.602400</td>\n",
       "      <td>0.583469</td>\n",
       "      <td>0.729243</td>\n",
       "      <td>0.795513</td>\n",
       "      <td>0.346586</td>\n",
       "      <td>0.410582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.572600</td>\n",
       "      <td>0.573997</td>\n",
       "      <td>0.732295</td>\n",
       "      <td>0.825429</td>\n",
       "      <td>0.310285</td>\n",
       "      <td>0.408910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.550600</td>\n",
       "      <td>0.557105</td>\n",
       "      <td>0.750305</td>\n",
       "      <td>0.786030</td>\n",
       "      <td>0.391530</td>\n",
       "      <td>0.498932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.528800</td>\n",
       "      <td>0.535247</td>\n",
       "      <td>0.765568</td>\n",
       "      <td>0.785032</td>\n",
       "      <td>0.450303</td>\n",
       "      <td>0.549894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.519900</td>\n",
       "      <td>0.521270</td>\n",
       "      <td>0.782967</td>\n",
       "      <td>0.776837</td>\n",
       "      <td>0.528954</td>\n",
       "      <td>0.610379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.507100</td>\n",
       "      <td>0.515042</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.786070</td>\n",
       "      <td>0.531547</td>\n",
       "      <td>0.617523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>0.506186</td>\n",
       "      <td>0.792125</td>\n",
       "      <td>0.821384</td>\n",
       "      <td>0.520311</td>\n",
       "      <td>0.620068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.516038</td>\n",
       "      <td>0.779304</td>\n",
       "      <td>0.843430</td>\n",
       "      <td>0.455488</td>\n",
       "      <td>0.579110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.488200</td>\n",
       "      <td>0.501854</td>\n",
       "      <td>0.790904</td>\n",
       "      <td>0.829144</td>\n",
       "      <td>0.508211</td>\n",
       "      <td>0.614964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.490900</td>\n",
       "      <td>0.491531</td>\n",
       "      <td>0.803114</td>\n",
       "      <td>0.808330</td>\n",
       "      <td>0.570441</td>\n",
       "      <td>0.649272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.476200</td>\n",
       "      <td>0.494487</td>\n",
       "      <td>0.800672</td>\n",
       "      <td>0.829808</td>\n",
       "      <td>0.539326</td>\n",
       "      <td>0.636204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.477900</td>\n",
       "      <td>0.490144</td>\n",
       "      <td>0.807387</td>\n",
       "      <td>0.821712</td>\n",
       "      <td>0.571305</td>\n",
       "      <td>0.656503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=765, training_loss=0.5195182541616602, metrics={'train_runtime': 465.9935, 'train_samples_per_second': 52.726, 'train_steps_per_second': 1.642, 'total_flos': 1608455646038016.0, 'train_loss': 0.5195182541616602, 'epoch': 14.926829268292684})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom loss function with label smoothing\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        # Apply label smoothing\n",
    "        labels = labels * (1 - self.smoothing) + 0.5 * self.smoothing\n",
    "        loss_fct = nn.BCEWithLogitsLoss()\n",
    "        return loss_fct(outputs, labels)\n",
    "\n",
    "label_smoothing_loss = LabelSmoothingLoss(smoothing=0.1)\n",
    "\n",
    "# Define Custom Loss Function in Trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.optimizer = None\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        if self.optimizer is None:\n",
    "            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.args.learning_rate)\n",
    "        return self.optimizer\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = label_smoothing_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(p):\n",
    "    preds = torch.sigmoid(torch.tensor(p.predictions)).cpu().numpy()\n",
    "    preds = (preds > 0.5).astype(int)  \n",
    "    labels = p.label_ids\n",
    "    accuracy = (preds == labels).mean()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=1)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the trainer with custom loss\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff8b3018-71ff-4dee-9eea-8be3e4276d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69/69 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4901435971260071,\n",
       " 'eval_accuracy': 0.8073870573870574,\n",
       " 'eval_precision': 0.8217118512667938,\n",
       " 'eval_recall': 0.5713050993949871,\n",
       " 'eval_f1': 0.6565025560188652,\n",
       " 'eval_runtime': 2.7432,\n",
       " 'eval_samples_per_second': 199.04,\n",
       " 'eval_steps_per_second': 25.153,\n",
       " 'epoch': 14.926829268292684}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d267af10-40f4-4a53-aa93-938305f847c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 1/1 [00:00<00:00, 16.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: a big way to end it if ever there was a dream australian open finals match-up the 2017 men's championship decider was \n",
      "surely it few could believe the ageing Roger Federer and Rafael Nadal had each won six matches to arrive at the big dance especially\n",
      "after injury interrupted seasons in 2016. Nadal had survived five setters against Alexander Zverev and Grigor Dimitrov to reach the final\n",
      "while federer had to see off ki Nishikori and Stan Vavrinka to earn his spot it was the spaniard who led thehead-to-head 23-11 but federer\n",
      "had the better record on hard courts with four ao crowns already under his belt we joined the action with nadal having opened up a three-love\n",
      "lead in the second set but they're starting to hit his strap she's making that shot in the first set wasn't he[Applause]the 15 from that side got to \n",
      "keep an eye on that shot \n",
      "\n",
      "\n",
      "Players: Grigor Dimitrov, Roger Federer, Stan Vavrinka, Rafael Nadal, Alexander Zverev\n",
      "Teams: \n",
      "Sentiment: Positive\n",
      "Associated Sport: tennis\n",
      "Predicted Labels: game outcome\n",
      "\n",
      "\n",
      "Label Probabilities:\n",
      "  game outcome: 0.5037\n",
      "  score-related: 0.4742\n",
      "  substitution/injury: 0.3214\n",
      "  assist/playmaking: 0.1269\n",
      "  defense actions: 0.0794\n",
      "  foul/penalty: 0.0712\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# new data\n",
    "new_texts = [\n",
    "'''a big way to end it if ever there was a dream australian open finals match-up the 2017 men's championship decider was \n",
    "surely it few could believe the ageing Roger Federer and Rafael Nadal had each won six matches to arrive at the big dance especially\n",
    "after injury interrupted seasons in 2016. Nadal had survived five setters against Alexander Zverev and Grigor Dimitrov to reach the final\n",
    "while federer had to see off ki Nishikori and Stan Vavrinka to earn his spot it was the spaniard who led thehead-to-head 23-11 but federer\n",
    "had the better record on hard courts with four ao crowns already under his belt we joined the action with nadal having opened up a three-love\n",
    "lead in the second set but they're starting to hit his strap she's making that shot in the first set wasn't he[Applause]the 15 from that side got to \n",
    "keep an eye on that shot '''\n",
    "\n",
    "    \n",
    "]\n",
    "\n",
    "# Function to preprocess and tokenize the new texts\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_and_tokenize(texts):\n",
    "    processed_texts = [preprocess_text(text) for text in texts]\n",
    "    tokenized_inputs = tokenizer(\n",
    "        processed_texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Regex pattern to match anomalous team names \n",
    "team_patterns = re.compile(r'\\b(?:[A-Z][a-z]+(?:\\s[A-Z][a-z]+))\\s(?:\\d+ers)\\b', re.IGNORECASE)\n",
    "\n",
    "# Function to extract entities (players and teams) \n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    players = set()\n",
    "    teams = set()\n",
    "\n",
    "    # List of articles to remove\n",
    "    articles = {\"the\", \"a\", \"an\", \"of\", \"for\"}\n",
    "\n",
    "    # Extract standard entities using NLP\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            players.add(ent.text)\n",
    "        elif ent.label_ in {\"ORG\", \"GPE\"}:\n",
    "           \n",
    "            filtered_team_name = \" \".join([token for token in ent.text.split() if token.lower() not in articles])\n",
    "            teams.add(filtered_team_name)\n",
    "\n",
    "    # Apply custom pattern matching for team names \n",
    "    matches = team_patterns.findall(text)\n",
    "    for match in matches:\n",
    "        # Remove any articles before the matched team names\n",
    "        match_tokens = match.split()\n",
    "        filtered_match = \" \".join([token for token in match_tokens if token.lower() not in articles])\n",
    "        teams.add(filtered_match.strip())\n",
    "\n",
    "    return list(players), list(teams)\n",
    "\n",
    "# Function to perform sentiment analysis on text\n",
    "def get_sentiment(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']\n",
    "\n",
    "# Updated function to determine the sport based only on contextual keywords\n",
    "def extract_sports(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    sports_priority = []\n",
    "    contextual_words_detected = {}  \n",
    "\n",
    "    # Extract sports based on contextual keywords\n",
    "    for final_sport, words in contextual_words.items():\n",
    "        matching_words = [word for word in words if word in text]\n",
    "        if len(matching_words) > 1: \n",
    "            sports_priority.append(final_sport)\n",
    "            contextual_words_detected[final_sport] = matching_words\n",
    "    \n",
    "    \n",
    "    if sports_priority:\n",
    "        return sports_priority[0]\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "\n",
    "new_inputs = preprocess_and_tokenize(new_texts).to(device)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculations for inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**new_inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, text in tqdm(enumerate(new_texts), total=len(new_texts), desc=\"Processing texts\"):\n",
    "    players, teams = extract_entities(text)\n",
    "    sentiment = get_sentiment(text)\n",
    "    label_probs = {}\n",
    "    predicted_labels = []\n",
    "\n",
    "    for j, label in enumerate(label_list):\n",
    "        custom_prob = probabilities[0][j] \n",
    "        label_probs[label] = custom_prob \n",
    "        # Determine predicted labels based on a threshold (e.g., 0.5)\n",
    "        if custom_prob > 0.5:\n",
    "            predicted_labels.append(label)\n",
    "\n",
    "    # Sort the label probabilities from highest to lowest\n",
    "    sorted_label_probs = {k: v for k, v in sorted(label_probs.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    sport = extract_sports(text)  \n",
    "\n",
    "    predictions.append({\n",
    "        \"Text\": text,\n",
    "        \"Players\": players,\n",
    "        \"Teams\": teams,\n",
    "        \"Sentiment\": sentiment,\n",
    "        \"Associated Sport\": sport,\n",
    "        \"Predicted Labels\": predicted_labels,\n",
    "        \"Label Probabilities\": sorted_label_probs\n",
    "    })\n",
    "    \n",
    "\n",
    "# Display the predictions along with probabilities and associated sports\n",
    "for prediction in predictions:\n",
    "    print(f\"Text: {prediction['Text']}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Players: {', '.join(prediction['Players'])}\")\n",
    "    print(f\"Teams: {', '.join(prediction['Teams'])}\")\n",
    "    print(f\"Sentiment: {'Positive' if prediction['Sentiment'] > 0 else 'Negative' if prediction['Sentiment'] < 0 else 'Neutral'}\")\n",
    "    print(f\"Associated Sport: {prediction['Associated Sport']}\")\n",
    "    print(f\"Predicted Labels: {', '.join(prediction['Predicted Labels'])}\")\n",
    "    print(\"\\n\")\n",
    "    print(\"Label Probabilities:\")\n",
    "    for label, prob in prediction[\"Label Probabilities\"].items():\n",
    "        print(f\"  {label}: {prob:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b472e-45d1-4efb-bb2a-2c2b738199f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general commentary\n",
    "\n",
    "'''Smith is off to a strong start, leading the pack as they approach the first bend. He’s looking confident, \n",
    "maintaining a good rhythm over the hurdles. But wait—he seems to be drifting out of his lane! \n",
    "Smith is losing track! Oh no, he's clipped the hurdle! He’s stumbled, and now he's losing his balance! \n",
    "This is disastrous! The other runners are speeding past him. What a heartbreaking turn of events for Smith—he was in such great form, \n",
    "but one small mistake has cost him the race'''\n",
    "\n",
    "'''With just under two minutes left in the fourth quarter, the Philadelphia 76ers are down by three. \n",
    "Embiid grabs the rebound off a missed shot and quickly outlets to Harden. Harden moves up the court, orchestrating the offense.\n",
    "He dribbles around the perimeter, drawing in the defense with a quick crossover. There’s a pick set by Tucker, giving Harden the space he needs. \n",
    "He drives into the lane, but at the last second, dishes a perfect no-look pass to Maxey in the corner. Maxey sets his feet and fires a three... \n",
    "It's good! What a brilliant playmaking sequence by Harden! The Philadelphia 76ers tie the game, and the crowd is roaring! \n",
    "This is why Harden is considered one of the best playmakers in the game.'''\n",
    "\n",
    "'''It's the second quarter of the game between the Washington Redskins and the New York Giants. Joe Theismann takes the snap,\n",
    "looking for an open receiver. Here comes Taylor—he breaks through the line, and... oh no! A devastating hit from Lawrence Taylor! \n",
    "Theismann is down! He’s not getting up. The replay shows it all—Taylor came in hard, and Theismann’s leg is caught awkwardly under \n",
    "the tackle. That leg just bent in a way it shouldn’t. The medical staff is rushing onto the field, and the players are visibly shaken.\n",
    "This is a serious injury. Theismann is being placed on a stretcher; he looks to be in excruciating pain. This could be career-ending. \n",
    "The crowd is silent, and even Taylor is signaling for the medical team. What a tragic moment for Theismann and the Redskins—an iconic \n",
    "player taken down by a horrific injury. '''\n",
    "\n",
    "'''It's third and short for the Browns, and they’re looking to convert. The ball is snapped, and here comes Harrison, \n",
    "blitzing off the edge with incredible speed! The quarterback hands it off to the running back, but Harrison explodes through the line—BOOM!\n",
    "What a tackle! He drives the runner back with a bone-crunching hit, stopping him dead in his tracks! \n",
    "You can hear the impact from the stands—Harrison's timing and power on that tackle were absolutely perfect. \n",
    "The ball carrier is down, and he looks shaken up after that hit. The Steelers’ defense comes up big on third down, \n",
    "thanks to a monstrous tackle by James Harrison! '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b595c-7223-4037-b96d-11f158c71074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#live commentary transcripts\n",
    "'''James catches puts up a three won't gorebound Bosh back out to Allen history point of the thing tie game with five seconds\n",
    "remaining Spurs do not have a timeout Parker to Neal four to shoot Arkansas who is ready to dunk for anyone so you can kind of see \n",
    "how it's affecting Tony Parker oh no we just saw me play them Cowboy Rose misses another.'''\n",
    "\n",
    "'''a big way to end it if ever there was a dream australian open finals match-up the 2017 men's championship decider was \n",
    "surely it few could believe the ageing Roger Federer and Rafael Nadal had each won six matches to arrive at the big dance especially\n",
    "after injury interrupted seasons in 2016. Nadal had survived five setters against Alexander Zverev and Grigor Dimitrov to reach the final\n",
    "while federer had to see off ki Nishikori and Stan Vavrinka to earn his spot it was the spaniard who led thehead-to-head 23-11 but federer\n",
    "had the better record on hard courts with four ao crowns already under his belt we joined the action with nadal having opened up a three-love\n",
    "lead in the second set but they're starting to hit his strap she's making that shot in the first set wasn't he[Applause]the 15 from that side got to \n",
    "keep an eye on that shot '''\n",
    "\n",
    "'''look we're looking to sweep you guys you wanted us you were crying out that you by passed the the harder team in my roast came down \n",
    "Brennan's left foot see him holding on to his knee holding on to his knee and down he was flying and he came down wrong on the left foot now \n",
    "whether it was an ankle or knee I do not know Gallants out there all those tweets running in this with the injury we just\n",
    "talked about this teams he is missed with an assortment of injuries and now holding a knee late in a game that it's already decided for\n",
    "all intents and purposes I'm sure everyone around the country is going to say wow why was he in it's in the game shall he comes down\n",
    "on the left leg keep an eye on the leftleg there yep it was winning when he was planted that's when whatever happened to happened it's\n",
    "here yes before he comes down it's the plant right there on that left leg if there was some give on that knee '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac3317-0921-4996-abf7-3de6b7bef1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigger text\n",
    "'''he's charged for the single no there's always drama now they'll go for the single now she got that one's got a 100 or \n",
    "hashe's got to wait to see if you're not cool he made his ground he thinks he has[Music]and that's how he plays as Shikhar Dhawan\n",
    "hoyt's one on to the leg side it's atough catch but easily taken on theboundarythere you gofor the momenthe's got 51 day international 50s\n",
    "dead straight absolutely dead straight he's got x that's out of the middle doesn't get it [Applause] that's a good shot that is just full\n",
    "of class [Applause] that's huge absolutely huge conceded in a world up well that is aamazing court and bold one-handed not in his arc \n",
    "doesn't get the power ms though he hits the toe of the bat that's very good this time over covers he finds the fielder Virat Kohli has\n",
    "remained off strike for a long period of time[Applause]finds the get Raul that is brilliant what a magnificent tribals he's faced\n",
    " for 11 runsand India they bring up 350 Australia will need 353 for victory Australia of course defending champions and this is what \n",
    " they need to win brilliant shots brilliant from Finch[Applause]that sounded good off the bat they'd be looking for two but uh oh\n",
    " we just about get it just about get it[Applause]that's it hard stayed on the back foot the ball was slow he's found the gap for four \n",
    " outside edge the third man fielder doesn't get it[Applause]well that is probably one of the slowest 50s maybe the slowest ever that's \n",
    " gone the distance[Applause]this time over midwicket that's exactly what i was saying it's against the breeze and Warner finds the man \n",
    " at deep mid wicket it's a nice shot there's nobody behind[Applause]that's going to be four that brings up the 50 for this and to man \n",
    " back like he did and that's as log sweep and that's going all the way they call him to deliverand boy it delivers every time oh that \n",
    " should be gone oh god this one coming back in as well he's given a chance some running to do it's a good take it's safe that's a good\n",
    " area to hit his neck down the ground and straight ball spinning back into Kerry as he picked him out he has he has it'sa long boundary \n",
    " carly agent gone diving forward Dhoni[Applause] Zampa will be the final wicket on the last ball of the game and that emotive man the \n",
    " captain of India tells us the story it's a win to india by 36 runs you'''\n",
    "\n",
    "\n",
    "'''Patrick Mahomes this after the touchdown hugs chiefs able to exhale power in light district back in kansas city going crazy with \n",
    "those great chiefs and sports fans[Applause]still two more time-outs left for the 49ers but down by 11. Garoppolo gets hit the ball \n",
    "comes outlooked like an incomplete pass but theylet them play on pressure by Frank Clark who just got the sack previously and it looked \n",
    "like an incomplete pass[Applause]i sure thought it was let's take a look at it it's good that yeah so yeah it's a forward pass a good \n",
    "jobby these officials letting it play out and garoppolo just trying to to buy sometime and let those guys get down the field where\n",
    "he can try to make a play but you know you mentioned Andy Reed Joe, Andone of the most respected and most liked guys in all of \n",
    "football and really the history of the game there's so many people that have been touched by him and what he has meant to him i know his \n",
    "players love him he loved him in philadelphia they loved him when he was coaching at green bay and they certainly love him in kansas city\n",
    "and he turned this program around second down in the very first year i know there's a lot of people really happyfor him to short of some \n",
    "miracle happening to come away with a super bowl win they did make the announcement that that last play was an incomplete pass not a fumble \n",
    "that kansas city chiefs super bowl four winning team 10 members of that team ended up in the hall of fame 17 total from the chiefs and \n",
    "the vikings that met at tulane stadium but how many players have come and gone great players great coaches for the chiefs that have not\n",
    "been able to taste this moment this is pit Kendall Fuller up to get it and that will end this game with under a minute to gohey\n",
    "patrick mahomes and the kansas city chiefs now with three straight post-season wins after trailing by 10 or more points[Applause]\n",
    "and Andy Reid gets to celebrate with his team timeout taken by Kyle Shanahan of the 49ers this moment belongs to kansas city the chiefs \n",
    "andy reid the hunt familybut  how about the job done on the other sideline a team that won four games last year the number one seed \n",
    "getting to the superbowl and coming up shortand kyle shanahan's third year as the head man with san francisco these are tough \n",
    "endings when you're notable to win the last game of the season you've had such a great year to get to this point this was a great \n",
    "season by san francisco a lot to be proud of but it will be hard for them to look at it that way after this game it'll take some time\n",
    "but for the kansas city chiefs hats off to them and the man known as big red is going to be a super bowl champ and big red just called\n",
    "his stud third year quarterback patrick mahomes a reigning mvp over to the sideline great quarterbacks win big games and\n",
    "patrick mahomes was bottled upmost of the night but came alive latemade plays on the move and he and these chiefs will walk out of here \n",
    "winners of superbowl 54.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826e085-b1d7-44e3-935a-05e68c86e85d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
